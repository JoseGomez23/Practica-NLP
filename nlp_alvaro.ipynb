{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dee0d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: readability-lxml in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (0.8.4.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from readability-lxml) (4.0.0)\n",
      "Requirement already satisfied: lxml[html_clean] in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from readability-lxml) (5.3.0)\n",
      "Requirement already satisfied: cssselect in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from readability-lxml) (1.2.0)\n",
      "Requirement already satisfied: lxml_html_clean in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from lxml[html_clean]->readability-lxml) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "# %pip install hf_xet\n",
    "# import torch\n",
    "# %pip install torch\n",
    "%pip install readability-lxml\n",
    "\n",
    "\n",
    "# # Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8524b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article_text):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    # BART tiene límite de tokens, cortar si el texto es muy largo\n",
    "    article_text = article_text[:1024]\n",
    "    \n",
    "    summary = summarizer(article_text, max_length=130, min_length=50, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d8772ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_classifier(data):\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Crear pipeline i definir model\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # Important si el text es en anglés, posar els topics també en anglés\n",
    "\n",
    "    # Definició dels topics\n",
    "    # Definició dels temes que pot tenir l'article\n",
    "    # possible_labels = [\"política\", \"deportes\", \"economía\", \"ciencia\", \"noticias\",\"tecnología\", \"salud\", \"cultura\", \"guerra\", \"internacional\"]\n",
    "    \n",
    "    possible_labels = [\"politics\", \"sports\", \"economy\", \"science\", \"technology\", \"health\", \"culture\", \"war\", \"international\"]\n",
    "\n",
    "\n",
    "    article_text = \" \".join(data)[:1024]\n",
    "    result = classifier(article_text, candidate_labels=possible_labels)\n",
    "\n",
    "    print(\"Topic més probable:\", result['labels'][0])\n",
    "    print(\"Scores:\", list(zip(result['labels'], result['scores'])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "45ad73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_article_preprocessing_a(clean_text):\n",
    "\n",
    "    # 1. Toeknize del texte\n",
    "    # tokens = word_tokenize(clean_text, language='spanish')\n",
    "    tokens = word_tokenize(clean_text, language='english')\n",
    "    \n",
    "    # 2. Normalitzar a minúscules i eliminar puntuació\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha()]  # solo palabras\n",
    "    \n",
    "    # 3. Eliminar stopweords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # print(tokens[:50])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def title_article_preprocessing_b(clean_text):\n",
    "    # ToDo: Implementar otra funcion de procesameinto de texto diferente \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2c56b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "from lxml import html\n",
    "\n",
    "url = \"https://link.springer.com/article/10.1186/s12915-020-00925-x\"\n",
    "\n",
    "\n",
    "\n",
    "def scrape_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # 1. Intentar <article>\n",
    "    article = soup.find(\"article\")\n",
    "    if article:\n",
    "        text = article.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        # 2. Intentar <main>\n",
    "        main = soup.find(\"main\")\n",
    "        if main:\n",
    "            text = main.get_text(separator=\"\\n\")\n",
    "        else:\n",
    "            # 3. Fallback: todos los <p>, unir el bloque más largo\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            blocks = []\n",
    "            current_block = []\n",
    "            for p in paragraphs:\n",
    "                current_block.append(p.get_text())\n",
    "                # Separar bloques grandes por lógica simple\n",
    "                if len(current_block) > 3:  \n",
    "                    blocks.append(\"\\n\".join(current_block))\n",
    "                    current_block = []\n",
    "            if current_block:\n",
    "                blocks.append(\"\\n\".join(current_block))\n",
    "            # Elegir el bloque más largo (probable artículo)\n",
    "            text = max(blocks, key=len) if blocks else \"\"\n",
    "    \n",
    "    # Limpiar líneas vacías\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    data = \"\\n\".join(lines)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def scrape_text_refined(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    tree = html.fromstring(response.content)\n",
    "\n",
    "    # Buscar el contenedor principal del artículo (para Springer suele ser c-article-body)\n",
    "    article_divs = tree.xpath('//div[contains(@class,\"c-article-body\")]')\n",
    "    \n",
    "    if article_divs:\n",
    "        paragraphs = article_divs[0].xpath('.//p//text()')\n",
    "        data = \"\\n\".join([p.strip() for p in paragraphs if p.strip()])\n",
    "    else:\n",
    "        # Fallback: todos los <p> de la página\n",
    "        paragraphs = tree.xpath('//p//text()')\n",
    "        data = \"\\n\".join([p.strip() for p in paragraphs if p.strip()])\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_1(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = huggingface_classifier(tokens)\n",
    "    resume = generate_summary(data)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_2(url):\n",
    "    data = scrape_text_refined(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = huggingface_classifier(tokens)\n",
    "    resume = generate_summary(data)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_3(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_b(data)\n",
    "    results = huggingface_classifier(tokens)\n",
    "    resume = generate_summary(data)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_4(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = train_model(tokens)\n",
    "    resume = generate_summary(data)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_5(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = huggingface_classifier(tokens)\n",
    "    resume = generate_summary_b(data)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5a0f561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Execution 1: URLhttps://link.springer.com/article/10.1186/s12915-020-00925-x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic més probable: science\n",
      "Scores: [('science', 0.6881455779075623), ('international', 0.07561494410037994), ('economy', 0.0668274536728859), ('culture', 0.0525495819747448), ('technology', 0.044419512152671814), ('health', 0.03472081199288368), ('war', 0.017296474426984787), ('sports', 0.01105914730578661), ('politics', 0.009366490878164768)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resum de l'article: Comparative analysis of morabine grasshopper genomes reveals highly abundant transposable elements and rapidly proliferating satellite DNA repeats. The study was published in the journal MCB Biology. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details.\n",
      "############# Execution 2: URLhttps://link.springer.com/article/10.1186/s12915-020-00925-x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic més probable: science\n",
      "Scores: [('science', 0.5759952068328857), ('international', 0.10380233079195023), ('economy', 0.08592421561479568), ('culture', 0.08271900564432144), ('technology', 0.07284616678953171), ('health', 0.03962034359574318), ('war', 0.01820867508649826), ('politics', 0.010658523067831993), ('sports', 0.010225552134215832)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resum de l'article: Repetitive DNA sequences, including transposable elements (TEs) and tandemly repeated satellite DNA (satDNAs), are found in high proportion in organisms across the Tree of Life. Grasshoppers have large genomes, averaging 9 Gb, that contain a high proportion of repetitive DNA, which has hampered progress in assembling reference genomes.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    urls = [\"https://link.springer.com/article/10.1186/s12915-020-00925-x\"]\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"############# Execution 1: URL{url}\")\n",
    "        execution_1(url)\n",
    "\n",
    "        print(f\"############# Execution 2: URL{url}\")\n",
    "        execution_2(url)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
