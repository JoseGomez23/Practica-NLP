{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee0d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: readability-lxml in c:\\users\\jose gómez\\anaconda3\\lib\\site-packages (0.8.4.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\jose gómez\\anaconda3\\lib\\site-packages (from readability-lxml) (4.0.0)\n",
      "Requirement already satisfied: lxml[html_clean] in c:\\users\\jose gómez\\anaconda3\\lib\\site-packages (from readability-lxml) (5.3.0)\n",
      "Requirement already satisfied: cssselect in c:\\users\\jose gómez\\anaconda3\\lib\\site-packages (from readability-lxml) (1.2.0)\n",
      "Requirement already satisfied: lxml_html_clean in c:\\users\\jose gómez\\anaconda3\\lib\\site-packages (from lxml[html_clean]->readability-lxml) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import trafilatura\n",
    "# %pip install hf_xet\n",
    "# import torch\n",
    "# %pip install torch\n",
    "%pip install readability-lxml\n",
    "\n",
    "\n",
    "# # Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8524b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_a(article_text):\n",
    "    summarizer = pipeline(\"text-generation\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    # BART tiene límite de tokens, cortar si el texto es muy largo\n",
    "    article_text = article_text[:1024]\n",
    "    \n",
    "    summary = summarizer(article_text, max_length=130, min_length=50, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8772ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_a(data):\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Crear pipeline i definir model\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # Important si el text es en anglés, posar els topics també en anglés\n",
    "\n",
    "    # Definició dels topics\n",
    "    # Definició dels temes que pot tenir l'article\n",
    "    # possible_labels = [\"política\", \"deportes\", \"economía\", \"ciencia\", \"noticias\",\"tecnología\", \"salud\", \"cultura\", \"guerra\", \"internacional\"]\n",
    "    \n",
    "    possible_labels = [\"politics\", \"sports\", \"economy\", \"science\", \"technology\", \"health\", \"culture\", \"war\", \"international\"]\n",
    "\n",
    "\n",
    "    article_text = \" \".join(data)[:1024]\n",
    "    result = classifier(article_text, candidate_labels=possible_labels, device=0)\n",
    "\n",
    "    print(\"Topic més probable:\", result['labels'][0])\n",
    "    print(\"Scores:\", list(zip(result['labels'], result['scores'])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45ad73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_article_preprocessing_a(clean_text):\n",
    "\n",
    "    # 1. Toeknize del texte\n",
    "    # tokens = word_tokenize(clean_text, language='spanish')\n",
    "    tokens = word_tokenize(clean_text, language='english')\n",
    "    \n",
    "    # 2. Normalitzar a minúscules i eliminar puntuació\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha()]  # solo palabras\n",
    "    \n",
    "    # 3. Eliminar stopweords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # print(tokens[:50])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def title_article_preprocessing_b(clean_text):\n",
    "    # ToDo: Implementar otra funcion de procesameinto de texto diferente \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c56b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "from lxml import html\n",
    "\n",
    "url = \"https://link.springer.com/article/10.1186/s12915-020-00925-x\"\n",
    "\n",
    "\n",
    "\n",
    "def scrape_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # 1. Intentar <article>\n",
    "    article = soup.find(\"article\")\n",
    "    if article:\n",
    "        text = article.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        # 2. Intentar <main>\n",
    "        main = soup.find(\"main\")\n",
    "        if main:\n",
    "            text = main.get_text(separator=\"\\n\")\n",
    "        else:\n",
    "            # 3. Fallback: todos los <p>, unir el bloque más largo\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            blocks = []\n",
    "            current_block = []\n",
    "            for p in paragraphs:\n",
    "                current_block.append(p.get_text())\n",
    "                # Separar bloques grandes por lógica simple\n",
    "                if len(current_block) > 3:  \n",
    "                    blocks.append(\"\\n\".join(current_block))\n",
    "                    current_block = []\n",
    "            if current_block:\n",
    "                blocks.append(\"\\n\".join(current_block))\n",
    "            # Elegir el bloque más largo (probable artículo)\n",
    "            text = max(blocks, key=len) if blocks else \"\"\n",
    "    \n",
    "    # Limpiar líneas vacías\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    data = \"\\n\".join(lines)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# def scrape_text_refined(url):\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()\n",
    "\n",
    "#     tree = html.fromstring(response.content)\n",
    "\n",
    "#     # Buscar el contenedor principal del artículo (para Springer suele ser c-article-body)\n",
    "#     article_divs = tree.xpath('//div[contains(@class,\"c-article-body\")]')\n",
    "    \n",
    "#     if article_divs:\n",
    "#         paragraphs = article_divs[0].xpath('.//p//text()')\n",
    "#         data = \"\\n\".join([p.strip() for p in paragraphs if p.strip()])\n",
    "#     else:\n",
    "#         # Fallback: todos los <p> de la página\n",
    "#         paragraphs = tree.xpath('//p//text()')\n",
    "#         data = \"\\n\".join([p.strip() for p in paragraphs if p.strip()])\n",
    "\n",
    "\n",
    "#     return data\n",
    "\n",
    "def scrape_text_advanced(url):\n",
    "    try:\n",
    "        # Descarregar la pàgina web amb un User-Agent vàlid\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        \n",
    "        # Forcem que la resposta sigui interpretada com a UTF-8\n",
    "        response.encoding = 'utf-8' \n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # 2. Li passem el contingut ja descarregat a trafilatura per només obtenir el cos del text, l'article.\n",
    "            clean_text = trafilatura.extract(response.text, include_comments=False)\n",
    "            \n",
    "            if clean_text:\n",
    "                print(clean_text[:1000])  # Mostrar només els primers 1000 caràcters\n",
    "                return clean_text\n",
    "        \n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la descarga: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4991fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_results = {}\n",
    "\n",
    "def execution_1(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = classifier_a(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 1)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_2(url):\n",
    "    data = scrape_text_advanced(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = classifier_a(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 2)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "# def execution_3(url):\n",
    "#     data = scrape_text(url)\n",
    "#     tokens = title_article_preprocessing_b(data)\n",
    "#     results = classifier_a(tokens)\n",
    "#     resume = generate_summary_a(data)\n",
    "\n",
    "#     save_results(results, 3)\n",
    "#     print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "# def execution_4(url):\n",
    "#     data = scrape_text(url)\n",
    "#     tokens = title_article_preprocessing_a(data)\n",
    "#     results = train_model(tokens)\n",
    "#     resume = generate_summary_a(data)\n",
    "\n",
    "#     save_results(results, 4)\n",
    "#     print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "# def execution_5(url):\n",
    "#     data = scrape_text(url)\n",
    "#     tokens = title_article_preprocessing_a(data)\n",
    "#     results = classifier_a(tokens)\n",
    "#     resume = generate_summary_b(data)\n",
    "\n",
    "#     save_results(results, 5)\n",
    "#     print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def save_results(results, num):\n",
    "        execution_results[f\"execution_{num}\"] = {\n",
    "        \"topic\": results['labels'][0],\n",
    "        \"score\": results['scores'][0]\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a0f561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Execution 1: URLhttps://link.springer.com/article/10.1186/s12915-020-00925-x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff3cae70b8344e6ba91a2330a4f2675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic més probable: science\n",
      "Scores: [('science', 0.6881455779075623), ('international', 0.07561494410037994), ('economy', 0.0668274536728859), ('culture', 0.0525495819747448), ('technology', 0.044419512152671814), ('health', 0.03472081199288368), ('war', 0.017296474426984787), ('sports', 0.01105914730578661), ('politics', 0.009366490878164768)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b4f7ec61db4c6d8ba78d9537586c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please make sure the generation config includes `forced_bos_token_id=0`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828f841141554cfca891af242b132863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BartForCausalLM LOAD REPORT from: facebook/bart-large-cnn\n",
      "Key                                                       | Status     |  | \n",
      "----------------------------------------------------------+------------+--+-\n",
      "model.encoder.layers.{0...11}.fc2.bias                    | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.k_proj.bias       | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.fc1.bias                    | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn_layer_norm.bias   | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn_layer_norm.weight | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.q_proj.weight     | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.out_proj.weight   | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.final_layer_norm.weight     | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.fc2.weight                  | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.final_layer_norm.bias       | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.v_proj.bias       | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.out_proj.bias     | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.q_proj.bias       | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.v_proj.weight     | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.self_attn.k_proj.weight     | UNEXPECTED |  | \n",
      "model.encoder.layers.{0...11}.fc1.weight                  | UNEXPECTED |  | \n",
      "model.encoder.embed_positions.weight                      | UNEXPECTED |  | \n",
      "model.encoder.layernorm_embedding.bias                    | UNEXPECTED |  | \n",
      "model.encoder.layernorm_embedding.weight                  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec059735cc124c93a046d4f05d8158d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc12f9f79288451f8bb1e2dd7cb9400d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8767940e0c748ada96e1ccea27c91c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f5841174b84c299944118d804b2b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Passing `generation_config` together with generation-related arguments=({'min_length', 'max_length', 'do_sample'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'summary_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m execution_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Topic - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Score - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m############# Execution 1: URL\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     execution_1(url)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m############# Execution 2: URL\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     execution_2(url)\n",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m, in \u001b[0;36mexecution_1\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m title_article_preprocessing_a(data)\n\u001b[0;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m classifier_a(tokens)\n\u001b[1;32m----> 7\u001b[0m resume \u001b[38;5;241m=\u001b[39m generate_summary_a(data)\n\u001b[0;32m      9\u001b[0m save_results(results, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResum de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m, in \u001b[0;36mgenerate_summary_a\u001b[1;34m(article_text)\u001b[0m\n\u001b[0;32m      5\u001b[0m article_text \u001b[38;5;241m=\u001b[39m article_text[:\u001b[38;5;241m1024\u001b[39m]\n\u001b[0;32m      7\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarizer(article_text, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m130\u001b[39m, min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'summary_text'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    urls = [\"https://link.springer.com/article/10.1186/s12915-020-00925-x\"]\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"############# Execution 1: URL{url}\")\n",
    "        execution_1(url)\n",
    "\n",
    "        print(f\"############# Execution 2: URL{url}\")\n",
    "        execution_2(url)\n",
    "\n",
    "    print(\"Execution Results Summary:\")\n",
    "    for key, value in execution_results.items():\n",
    "        print(f\"{key}: Topic - {value['topic']}, Score - {value['score']}\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (2.10.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (0.25.0+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (2.10.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
