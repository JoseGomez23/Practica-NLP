{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca806ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: readability-lxml in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (0.8.4.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from readability-lxml) (5.2.0)\n",
      "Requirement already satisfied: lxml[html_clean] in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from readability-lxml) (6.0.2)\n",
      "Requirement already satisfied: cssselect in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from readability-lxml) (1.4.0)\n",
      "Requirement already satisfied: lxml_html_clean in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from lxml[html_clean]->readability-lxml) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.10.0)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/torchvision-0.2.0-py2.py3-none-any.whl (48 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torchaudio (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for torchaudio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (1.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alvaro\\appdata\\roaming\\python\\python314\\site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alvaro\\appdata\\roaming\\python\\python314\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from typer-slim->transformers) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from nltk) (4.67.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\alvaro\\appdata\\roaming\\python\\python314\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alvaro\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install hf_xet\n",
    "# %pip install torch\n",
    "# %pip install readability-lxml\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install transformers\n",
    "# %pip install nltk\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install requests\n",
    "%pip install spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download ca_core_news_sm\n",
    "%pip install WordCloud\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers.pipelines import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import spacy\n",
    "from wordcloud import WordCloudf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "\n",
    "# # Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8524b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 511/511 [00:00<00:00, 1186.08it/s, Materializing param=model.encoder.layers.11.self_attn_layer_norm.weight]   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "def generate_summary_a(article_text):\n",
    "\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        article_text[:1024],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=130,\n",
    "        min_length=50,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8772ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 515/515 [00:00<00:00, 1210.97it/s, Materializing param=model.shared.weight]                                   \n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def classifier_a(data):\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Crear pipeline i definir model\n",
    "    # Important si el text es en anglés, posar els topics també en anglés\n",
    "\n",
    "    # Definició dels topics\n",
    "    # Definició dels temes que pot tenir l'article\n",
    "    # possible_labels = [\"política\", \"deportes\", \"economía\", \"ciencia\", \"noticias\",\"tecnología\", \"salud\", \"cultura\", \"guerra\", \"internacional\"]\n",
    "    \n",
    "    possible_labels = [\"politics\", \"sports\", \"economy\", \"science\", \"technology\", \"health\", \"culture\", \"war\", \"international\"]\n",
    "\n",
    "\n",
    "    article_text = \" \".join(data)[:1024]\n",
    "    result = classifier(article_text, candidate_labels=possible_labels, device=0)\n",
    "\n",
    "    print(\"Topic més probable:\", result['labels'][0])\n",
    "    print(\"Scores:\", list(zip(result['labels'], result['scores'])))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def classifier_b(tokens):\n",
    "    data = fetch_20newsgroups(subset='train', categories=[\"politics\", \"sports\", \"economy\", \"science\", \"news\", \"technology\", \"health\", \"culture\", \"war\", \"international\"])\n",
    "\n",
    "    model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "    model.fit(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfa8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(txt):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(txt)\n",
    "\n",
    "    # Mostrar el núvol de paraules\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # Ocultar ejes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_article_preprocessing_a(clean_text):\n",
    "\n",
    "    # 1. Toeknize del texte\n",
    "    # tokens = word_tokenize(clean_text, language='spanish')\n",
    "    tokens = word_tokenize(clean_text, language='english')\n",
    "    \n",
    "    # 2. Normalitzar a minúscules i eliminar puntuació\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha()]  # solo palabras\n",
    "    \n",
    "    # 3. Eliminar stopweords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # print(tokens[:50])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def title_article_preprocessing_b(clean_text):\n",
    "    # Carrega del model\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Processament\n",
    "    doc = nlp(clean_text)\n",
    "    \n",
    "    # 1. Neteja bàsica amb spaCy (Eliminar stop words i puntuació)\n",
    "    tokens_limpios = [\n",
    "    token.text.strip() \n",
    "    for token in doc \n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "    ]\n",
    "\n",
    "    # Per si de cas queda algun string buit després del strip()\n",
    "    tokens_limpios = [t for t in tokens_limpios if t]\n",
    "\n",
    "    print(f\"Primers 20 tokens nets (Opció B): {tokens_limpios[:20]}\")\n",
    "    show_wordcloud(' '.join(tokens_limpios))\n",
    "\n",
    "    return tokens_limpios\n",
    "    train_model(tokens_limpios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c56b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "from lxml import html\n",
    "\n",
    "url = \"https://link.springer.com/article/10.1186/s12915-020-00925-x\"\n",
    "\n",
    "\n",
    "\n",
    "def scrape_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # 1. Intentar <article>\n",
    "    article = soup.find(\"article\")\n",
    "    if article:\n",
    "        text = article.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        # 2. Intentar <main>\n",
    "        main = soup.find(\"main\")\n",
    "        if main:\n",
    "            text = main.get_text(separator=\"\\n\")\n",
    "        else:\n",
    "            # 3. Fallback: todos los <p>, unir el bloque más largo\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            blocks = []\n",
    "            current_block = []\n",
    "            for p in paragraphs:\n",
    "                current_block.append(p.get_text())\n",
    "                # Separar bloques grandes por lógica simple\n",
    "                if len(current_block) > 3:  \n",
    "                    blocks.append(\"\\n\".join(current_block))\n",
    "                    current_block = []\n",
    "            if current_block:\n",
    "                blocks.append(\"\\n\".join(current_block))\n",
    "            # Elegir el bloque más largo (probable artículo)\n",
    "            text = max(blocks, key=len) if blocks else \"\"\n",
    "    \n",
    "    # Limpiar líneas vacías\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    data = \"\\n\".join(lines)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def scrape_text_refined(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    tree = html.fromstring(response.content)\n",
    "\n",
    "    # Buscar el contenedor principal del artículo (para Springer suele ser c-article-body)\n",
    "    article_divs = tree.xpath('//div[contains(@class,\"c-article-body\")]')\n",
    "    \n",
    "    if article_divs:\n",
    "        paragraphs = article_divs[0].xpath('.//p//text()')\n",
    "        data = \"\\n\".join([p.strip() for p in paragraphs if p.strip()])\n",
    "    else:\n",
    "        # Fallback: todos los <p> de la página\n",
    "        paragraphs = tree.xpath('//p//text()')\n",
    "        data = \"\\n\".join([p.strip() for p in paragraphs if p.strip()])\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_results = {}\n",
    "\n",
    "def execution_1(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = classifier_a(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 1)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_2(url):\n",
    "    data = scrape_text_refined(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = classifier_a(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 2)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_3(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_b(data)\n",
    "    results = classifier_a(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 3)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_4(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = classifier_b(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 4)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "# def execution_5(url):\n",
    "#     data = scrape_text(url)\n",
    "#     tokens = title_article_preprocessing_a(data)\n",
    "#     results = classifier_a(tokens)\n",
    "#     resume = generate_summary_b(data)\n",
    "\n",
    "#     save_results(results, 5)\n",
    "#     print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def save_results(results, num):\n",
    "        execution_results[f\"execution_{num}\"] = {\n",
    "        \"topic\": results['labels'][0],\n",
    "        \"score\": results['scores'][0]\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a0f561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Execution 1: URLhttps://link.springer.com/article/10.1186/s12915-020-00925-x\n",
      "Topic més probable: science\n",
      "Scores: [('science', 0.6881457567214966), ('international', 0.07561497390270233), ('economy', 0.06682740151882172), ('culture', 0.05254959687590599), ('technology', 0.044419459998607635), ('health', 0.03472079336643219), ('war', 0.017296485602855682), ('sports', 0.011059140786528587), ('politics', 0.009366471320390701)]\n",
      "Resum de l'article: BMC Biology. Comparative analysis of morabine grasshopper genomes reveals highly abundant transposable elements and rapidly proliferating satellite DNA repeats. Findings published in article number number number number 199 (2020) and article number 18 (18)\n",
      "############# Execution 2: URLhttps://link.springer.com/article/10.1186/s12915-020-00925-x\n",
      "Topic més probable: science\n",
      "Scores: [('science', 0.5759950876235962), ('international', 0.10380254685878754), ('economy', 0.08592448383569717), ('culture', 0.08271884173154831), ('technology', 0.0728459432721138), ('health', 0.039620332419872284), ('war', 0.018208736553788185), ('politics', 0.010658500716090202), ('sports', 0.010225575417280197)]\n",
      "Resum de l'article: Repetitive DNA sequences, including transposable elements (TEs) and tandemly repeated satellite DNA (satDNAs), are found in high proportion in organisms across the Tree of Life. Grasshoppers have large genomes, averaging 9 Gb, that contain a high proportion of repetitive DNA, which has hampered progress in assembling reference genomes.\n",
      "Execution Results Summary:\n",
      "execution_1: Topic - science, Score - 0.6881457567214966\n",
      "execution_2: Topic - science, Score - 0.5759950876235962\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    urls = [\"https://link.springer.com/article/10.1186/s12915-020-00925-x\"]\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"############# Execution 1: URL{url}\")\n",
    "        execution_1(url)\n",
    "\n",
    "        print(f\"############# Execution 2: URL{url}\")\n",
    "        execution_2(url)\n",
    "\n",
    "    print(\"Execution Results Summary:\")\n",
    "    for key, value in execution_results.items():\n",
    "        print(f\"{key}: Topic - {value['topic']}, Score - {value['score']}\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "905eb9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
