{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee0d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: readability-lxml in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (0.8.4.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from readability-lxml) (4.0.0)\n",
      "Requirement already satisfied: lxml[html_clean] in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from readability-lxml) (5.3.0)\n",
      "Requirement already satisfied: cssselect in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from readability-lxml) (1.2.0)\n",
      "Requirement already satisfied: lxml_html_clean in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from lxml[html_clean]->readability-lxml) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install hf_xet\n",
    "# %pip install torch\n",
    "# %pip install readability-lxml\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install transformers\n",
    "# %pip install nltk\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install requests\n",
    "%pip install spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download ca_core_news_sm\n",
    "%pip install WordCloud\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "# %pip install hf_xet\n",
    "# import torch\n",
    "# %pip install torch\n",
    "%pip install readability-lxml\n",
    "\n",
    "\n",
    "# # Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8524b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 511/511 [00:00<00:00, 1186.08it/s, Materializing param=model.encoder.layers.11.self_attn_layer_norm.weight]   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "def generate_summary_a(article_text):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        article_text[:1024],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=130,\n",
    "        min_length=50,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8772ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 515/515 [00:00<00:00, 1210.97it/s, Materializing param=model.shared.weight]                                   \n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def classifier_a(data):\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Crear pipeline i definir model\n",
    "    # Important si el text es en anglés, posar els topics també en anglés\n",
    "\n",
    "    # Definició dels topics\n",
    "    # Definició dels temes que pot tenir l'article\n",
    "    # possible_labels = [\"política\", \"deportes\", \"economía\", \"ciencia\", \"noticias\",\"tecnología\", \"salud\", \"cultura\", \"guerra\", \"internacional\"]\n",
    "    \n",
    "    possible_labels = [\"politics\", \"sports\", \"economy\", \"science\", \"technology\", \"health\", \"culture\", \"war\", \"international\"]\n",
    "\n",
    "\n",
    "    article_text = \" \".join(data)[:1024]\n",
    "    result = classifier(article_text, candidate_labels=possible_labels, device=0)\n",
    "\n",
    "    print(\"Topic més probable:\", result['labels'][0])\n",
    "    print(\"Scores:\", list(zip(result['labels'], result['scores'])))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def classifier_b(tokens):\n",
    "    data = fetch_20newsgroups(subset='train', categories=[\"politics\", \"sports\", \"economy\", \"science\", \"news\", \"technology\", \"health\", \"culture\", \"war\", \"international\"])\n",
    "\n",
    "    model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "    model.fit(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfa8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(txt):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(txt)\n",
    "\n",
    "    # Mostrar el núvol de paraules\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # Ocultar ejes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ad73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_article_preprocessing_a(clean_text):\n",
    "\n",
    "    # 1. Toeknize del texte\n",
    "    # tokens = word_tokenize(clean_text, language='spanish')\n",
    "    tokens = word_tokenize(clean_text, language='english')\n",
    "    \n",
    "    # 2. Normalitzar a minúscules i eliminar puntuació\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha()]  # solo palabras\n",
    "    \n",
    "    # 3. Eliminar stopweords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # print(tokens[:50])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def title_article_preprocessing_b(clean_text):\n",
    "    # Carrega del model\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Processament\n",
    "    doc = nlp(clean_text)\n",
    "    \n",
    "    # 1. Neteja bàsica amb spaCy (Eliminar stop words i puntuació)\n",
    "    tokens_limpios = [\n",
    "    token.text.strip() \n",
    "    for token in doc \n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "    ]\n",
    "\n",
    "    # Per si de cas queda algun string buit després del strip()\n",
    "    tokens_limpios = [t for t in tokens_limpios if t]\n",
    "\n",
    "    print(f\"Primers 20 tokens nets (Opció B): {tokens_limpios[:20]}\")\n",
    "    show_wordcloud(' '.join(tokens_limpios))\n",
    "\n",
    "    return tokens_limpios\n",
    "    train_model(tokens_limpios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c56b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "from lxml import html\n",
    "\n",
    "url = \"https://link.springer.com/article/10.1186/s12915-020-00925-x\"\n",
    "\n",
    "\n",
    "\n",
    "def scrape_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # 1. Intentar <article>\n",
    "    article = soup.find(\"article\")\n",
    "    if article:\n",
    "        text = article.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        # 2. Intentar <main>\n",
    "        main = soup.find(\"main\")\n",
    "        if main:\n",
    "            text = main.get_text(separator=\"\\n\")\n",
    "        else:\n",
    "            # 3. Fallback: todos los <p>, unir el bloque más largo\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            blocks = []\n",
    "            current_block = []\n",
    "            for p in paragraphs:\n",
    "                current_block.append(p.get_text())\n",
    "                # Separar bloques grandes por lógica simple\n",
    "                if len(current_block) > 3:  \n",
    "                    blocks.append(\"\\n\".join(current_block))\n",
    "                    current_block = []\n",
    "            if current_block:\n",
    "                blocks.append(\"\\n\".join(current_block))\n",
    "            # Elegir el bloque más largo (probable artículo)\n",
    "            text = max(blocks, key=len) if blocks else \"\"\n",
    "    \n",
    "    # Limpiar líneas vacías\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    data = \"\\n\".join(lines)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# def scrape_text_refined(url):\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()\n",
    "\n",
    "#     tree = html.fromstring(response.content)\n",
    "\n",
    "#     # Buscar el contenedor principal del artículo (para Springer suele ser c-article-body)\n",
    "#     article_divs = tree.xpath('//div[contains(@class,\"c-article-body\")]')\n",
    "    \n",
    "#     if article_divs:\n",
    "#         paragraphs = article_divs[0].xpath('.//p//text()')\n",
    "#         data = \"\\n\".join([p.strip() for p in paragraphs if p.strip()])\n",
    "#     else:\n",
    "#         # Fallback: todos los <p> de la página\n",
    "#         paragraphs = tree.xpath('//p//text()')\n",
    "#         data = \"\\n\".join([p.strip() for p in paragraphs if p.strip()])\n",
    "\n",
    "\n",
    "#     return data\n",
    "\n",
    "def scrape_text_advanced(url):\n",
    "    try:\n",
    "        # Descarregar la pàgina web amb un User-Agent vàlid\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        \n",
    "        # Forcem que la resposta sigui interpretada com a UTF-8\n",
    "        response.encoding = 'utf-8' \n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # 2. Li passem el contingut ja descarregat a trafilatura per només obtenir el cos del text, l'article.\n",
    "            clean_text = trafilatura.extract(response.text, include_comments=False)\n",
    "            \n",
    "            if clean_text:\n",
    "                print(clean_text[:1000])  # Mostrar només els primers 1000 caràcters\n",
    "                return clean_text\n",
    "        \n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la descarga: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4991fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_results = {}\n",
    "\n",
    "def execution_1(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = classifier_a(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 1)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_2(url):\n",
    "    data = scrape_text_advanced(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = classifier_a(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 2)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_3(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_b(data)\n",
    "    results = classifier_a(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 3)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def execution_4(url):\n",
    "    data = scrape_text(url)\n",
    "    tokens = title_article_preprocessing_a(data)\n",
    "    results = classifier_b(tokens)\n",
    "    resume = generate_summary_a(data)\n",
    "\n",
    "    save_results(results, 4)\n",
    "    print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "# def execution_5(url):\n",
    "#     data = scrape_text(url)\n",
    "#     tokens = title_article_preprocessing_a(data)\n",
    "#     results = classifier_a(tokens)\n",
    "#     resume = generate_summary_b(data)\n",
    "\n",
    "#     save_results(results, 5)\n",
    "#     print(f\"Resum de l'article: {resume}\")\n",
    "\n",
    "\n",
    "def save_results(results, num):\n",
    "        execution_results[f\"execution_{num}\"] = {\n",
    "        \"topic\": results['labels'][0],\n",
    "        \"score\": results['scores'][0]\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0f561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Execution 1: URLhttps://link.springer.com/article/10.1186/s12915-020-00925-x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic més probable: science\n",
      "Scores: [('science', 0.6881455779075623), ('international', 0.07561494410037994), ('economy', 0.0668274536728859), ('culture', 0.0525495819747448), ('technology', 0.044419512152671814), ('health', 0.03472081199288368), ('war', 0.017296474426984787), ('sports', 0.01105914730578661), ('politics', 0.009366490878164768)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resum de l'article: Comparative analysis of morabine grasshopper genomes reveals highly abundant transposable elements and rapidly proliferating satellite DNA repeats. The study was published in the journal MCB Biology. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details.\n",
      "############# Execution 2: URLhttps://link.springer.com/article/10.1186/s12915-020-00925-x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic més probable: science\n",
      "Scores: [('science', 0.5759952068328857), ('international', 0.10380233079195023), ('economy', 0.08592421561479568), ('culture', 0.08271900564432144), ('technology', 0.07284616678953171), ('health', 0.03962034359574318), ('war', 0.01820867508649826), ('politics', 0.010658523067831993), ('sports', 0.010225552134215832)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resum de l'article: Repetitive DNA sequences, including transposable elements (TEs) and tandemly repeated satellite DNA (satDNAs), are found in high proportion in organisms across the Tree of Life. Grasshoppers have large genomes, averaging 9 Gb, that contain a high proportion of repetitive DNA, which has hampered progress in assembling reference genomes.\n",
      "Execution Results Summary:\n",
      "execution_1: Topic - science, Score - 0.6881455779075623\n",
      "execution_2: Topic - science, Score - 0.5759952068328857\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    urls = [\"https://link.springer.com/article/10.1186/s12915-020-00925-x\"]\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"############# Execution 1: URL{url}\")\n",
    "        execution_1(url)\n",
    "\n",
    "        print(f\"############# Execution 2: URL{url}\")\n",
    "        execution_2(url)\n",
    "\n",
    "    print(\"Execution Results Summary:\")\n",
    "    for key, value in execution_results.items():\n",
    "        print(f\"{key}: Topic - {value['topic']}, Score - {value['score']}\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (2.10.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (0.25.0+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (2.10.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "905eb9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())  \u001b[38;5;66;03m# Debe devolver True\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Alvaro\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:599\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_name\u001b[39m(device: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    588\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \n\u001b[0;32m    590\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_device_properties(device)\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\Alvaro\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:632\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_properties\u001b[39m(device: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    621\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 632\u001b[0m     _lazy_init()  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32mc:\\Users\\Alvaro\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:417\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Debe devolver True\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de tu GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
