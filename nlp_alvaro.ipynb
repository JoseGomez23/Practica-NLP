{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da624fd7",
   "metadata": {},
   "source": [
    "# 1. Agafar el text d'una web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dee0d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45ad73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_article_preprocessing_a(clean_text):\n",
    "    # 1. Tokenizar el texto\n",
    "    tokens = word_tokenize(clean_text, language='spanish')\n",
    "    \n",
    "    # 2. Normalizar: pasar a minúsculas y quitar puntuación\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha()]  # solo palabras\n",
    "    \n",
    "    # 3. Eliminar palabras vacías (stopwords)\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    print(tokens[:50])  # mostrar los primeros 50 tokens procesados\n",
    "\n",
    "\n",
    "def title_article_preprocessing_b(clean_text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c56b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home\n",
      "BMC Biology\n",
      "Article\n",
      "Comparative analysis of morabine grasshopper genomes reveals highly abundant transposable elements and rapidly proliferating satellite DNA repeats\n",
      "Research article\n",
      "Open access\n",
      "Published:\n",
      "21 December 2020\n",
      "Volume 18\n",
      ", article number\n",
      "199\n",
      ", (\n",
      "2020\n",
      ")\n",
      "Cite this article\n",
      "You have full access to this\n",
      "open access\n",
      "article\n",
      "Download PDF\n",
      "Save article\n",
      "View saved research\n",
      "BMC Biology\n",
      "Aims and scope\n",
      "Submit manuscript\n",
      "Comparative analysis of morabine grasshopper genomes reveals highly abundant transposable elements and rapidly proliferating satellite DNA repeats\n",
      "Download PDF\n",
      "Octavio M. Palacios-Gimenez\n",
      "ORCID:\n",
      "orcid.org/0000-0002-1472-9949\n",
      "1\n",
      ",\n",
      "2\n",
      ",\n",
      "Julia Koelman\n",
      "1\n",
      ",\n",
      "Marc Palmada-Flores\n",
      "1\n",
      ",\n",
      "Tessa M. Bradford\n",
      "3\n",
      ",\n",
      "4\n",
      ",\n",
      "Karl K. Jones\n",
      "3\n",
      ",\n",
      "Steven J. B. Cooper\n",
      "3\n",
      ",\n",
      "4\n",
      ",\n",
      "Takeshi Kawakami\n",
      "1\n",
      ",\n",
      "5\n",
      "na1\n",
      "&\n",
      "…\n",
      "Alexander Suh\n",
      "1\n",
      ",\n",
      "2\n",
      ",\n",
      "6\n",
      "na1\n",
      "Show authors\n",
      "6436\n",
      "Accesses\n",
      "47\n",
      "Citations\n",
      "12\n",
      "Altmetric\n",
      "Explore all metrics\n",
      "Abstract\n",
      "Background\n",
      "Repetitive DNA sequences, including transposable elements (TEs) and tand\n",
      "['home', 'bmc', 'biology', 'article', 'comparative', 'analysis', 'of', 'morabine', 'grasshopper', 'genomes', 'reveals', 'highly', 'abundant', 'transposable', 'elements', 'and', 'rapidly', 'proliferating', 'satellite', 'dna', 'repeats', 'research', 'article', 'open', 'access', 'published', 'december', 'volume', 'article', 'number', 'cite', 'this', 'article', 'you', 'have', 'full', 'access', 'to', 'this', 'open', 'access', 'article', 'download', 'pdf', 'save', 'article', 'view', 'saved', 'research', 'bmc']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://link.springer.com/article/10.1186/s12915-020-00925-x\"\n",
    "\n",
    "opcio = input(\"Selecciona opció A o B\")\n",
    "\n",
    "\n",
    "def scrape_text(url, opcio):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # 1. Intentar <article>\n",
    "    article = soup.find(\"article\")\n",
    "    if article:\n",
    "        text = article.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        # 2. Intentar <main>\n",
    "        main = soup.find(\"main\")\n",
    "        if main:\n",
    "            text = main.get_text(separator=\"\\n\")\n",
    "        else:\n",
    "            # 3. Fallback: todos los <p>, unir el bloque más largo\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            blocks = []\n",
    "            current_block = []\n",
    "            for p in paragraphs:\n",
    "                current_block.append(p.get_text())\n",
    "                # Separar bloques grandes por lógica simple\n",
    "                if len(current_block) > 3:  \n",
    "                    blocks.append(\"\\n\".join(current_block))\n",
    "                    current_block = []\n",
    "            if current_block:\n",
    "                blocks.append(\"\\n\".join(current_block))\n",
    "            # Elegir el bloque más largo (probable artículo)\n",
    "            text = max(blocks, key=len) if blocks else \"\"\n",
    "    \n",
    "    # Limpiar líneas vacías\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    clean_text = \"\\n\".join(lines)\n",
    "    \n",
    "\n",
    "    print(clean_text[:1000])  # Mostrar solo los primeros 1000 caracteres\n",
    "    if opcio == \"A\":\n",
    "        title_article_preprocessing_a(clean_text)\n",
    "    elif opcio == \"B\":\n",
    "        title_article_preprocessing_b(clean_text)\n",
    "    else:\n",
    "        print(\"Opció no vàlida\")\n",
    "\n",
    "\n",
    "scrape_text(url, opcio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f561c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
