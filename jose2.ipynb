{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da624fd7",
   "metadata": {},
   "source": [
    "# 1. Agafar el text d'una web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jose\n",
      "[nltk_data]     Gómez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install spacy\n",
    "# !python -m spacy download es_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download ca_core_news_sm\n",
    "# %pip install WordCloud\n",
    "# !pip install --upgrade transformers tokenizers sentencepiece\n",
    "# !pip install sentencepiece\n",
    "# !pip install trafilatura\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup   \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer           #eina de stemming              #diccionari de freqüències\n",
    "from wordcloud import WordCloud                 #eina de visualització\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e279012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "def generate_summary_b(article_text):\n",
    "    \n",
    "    model_name = \"t5-base\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    # Prefix obligatori per a T5\n",
    "    input_text = f\"summarize: {article_text.strip()}\"\n",
    "    \n",
    "    # Tokenització i codificació\n",
    "    inputs = tokenizer(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=512, \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            num_beams=4,\n",
    "            max_length=150,\n",
    "            min_length=40,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    # Decodificació\n",
    "    resumen = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    logits = torch.stack(outputs.scores, dim=1) \n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Tokens generats (excloent el token inicial)\n",
    "    gen_tokens = outputs.sequences[:, 1:1 + logits.shape[1]]\n",
    "    \n",
    "    # Obtenir probabilitats de tokens generats\n",
    "    gen_probs = torch.gather(probs, 2, gen_tokens.unsqueeze(-1))\n",
    "    confianza = torch.mean(gen_probs).item()\n",
    "\n",
    "    print(f\"Confianza del resumen: {confianza:.4f}\")\n",
    "    print(f\"Resumen generado: {resumen}\")\n",
    "    return resumen, confianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f779244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Ex 3\n",
    "def train_model(tokens):\n",
    "    # Càrrega de dades\n",
    "    data = fetch_20newsgroups(subset='train', categories=[\n",
    "        'sci.med', 'sci.space', 'sci.electronics', 'sci.crypt',\n",
    "        'rec.autos', 'rec.sport.hockey', 'comp.graphics',\n",
    "        'talk.politics.misc', 'soc.religion.christian', 'misc.forsale'\n",
    "    ])\n",
    "\n",
    "    # 2. Entrenament\n",
    "    modelo_entrenado = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "    modelo_entrenado.fit(data.data, data.target)\n",
    "    nombres_categorias = data.target_names\n",
    "\n",
    "    # Processament article tokenitzat\n",
    "    tokenized_string = \" \".join(tokens)\n",
    "\n",
    "    # Predicció\n",
    "    prediccion_indice = modelo_entrenado.predict([tokenized_string])\n",
    "\n",
    "    # Resultat\n",
    "    categoria_final = nombres_categorias[prediccion_indice[0]]\n",
    "    \n",
    "    print(f\"Classificat com: {categoria_final}\")\n",
    "    return categoria_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impressio paraules més freqüents\n",
    "def show_wordcloud(txt):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(txt)\n",
    "\n",
    "    # Mostrar el núvol de paraules\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex 2\n",
    "\n",
    "def title_article_preprocessing_a(clean_text):\n",
    "    # Tokenitzar text\n",
    "    tokens = word_tokenize(clean_text, language='spanish')\n",
    "    \n",
    "    # Normalitzar text\n",
    "    tokens = [t.lower() for t in tokens if t.isalpha()]  # Nomès paraules\n",
    "    \n",
    "    # 3. Eliminar palabras vacías (stopwords)\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    print(tokens[:50])  # Mostrar primers 50 tokens\n",
    "\n",
    "\n",
    "def title_article_preprocessing_b(clean_text):\n",
    "    # Carrega del model\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Processament\n",
    "    doc = nlp(clean_text)\n",
    "    \n",
    "    # 1. Neteja bàsica amb spaCy (Eliminar stop words i puntuació)\n",
    "    tokens_limpios = [\n",
    "    token.text.strip() \n",
    "    for token in doc \n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "    ]\n",
    "\n",
    "    # Per si de cas queda algun string buit després del strip()\n",
    "    tokens_limpios = [t for t in tokens_limpios if t]\n",
    "\n",
    "    print(f\"Primers 20 tokens nets (Opció B): {tokens_limpios[:20]}\")\n",
    "    \n",
    "    show_wordcloud(' '.join(tokens_limpios))\n",
    "    train_model(tokens_limpios)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "Background\n",
      "Repetitive DNA sequences, including transposable elements (TEs) and tandemly repeated satellite DNA (satDNAs), collectively called the “repeatome”, are found in high proportion in organisms across the Tree of Life. Grasshoppers have large genomes, averaging 9 Gb, that contain a high proportion of repetitive DNA, which has hampered progress in assembling reference genomes. Here we combined linked-read genomics with transcriptomics to assemble, characterize, and compare the structure of repetitive DNA sequences in four chromosomal races of the morabine grasshopper Vandiemenella viatica species complex and determine their contribution to genome evolution.\n",
      "Results\n",
      "We obtained linked-read genome assemblies of 2.73–3.27 Gb from estimated genome sizes of 4.26–5.07 Gb DNA per haploid genome of the four chromosomal races of V. viatica. These constitute the third largest insect genomes assembled so far. Combining complementary annotation tools and manual curation, we found a \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a637ec020c2c474e8b3f840b19115dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confianza del resumen: 0.4397\n",
      "Resumen generado: grasshoppers have large genomes, averaging 9 Gb, that contain a high proportion of repetitive DNA . these sequences have hampered progress in assembling reference genomes . a large diversity of TEs and satDNAs, constituting 66 to 75% per genome assembly .\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://link.springer.com/article/10.1186/s12915-020-00925-x\"\n",
    "\n",
    "opcio = input(\"Selecciona opció A o B\")\n",
    "\n",
    "\n",
    "def scrape_text(url, opcio):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Intentar <article>\n",
    "    article = soup.find(\"article\")\n",
    "    if article:\n",
    "        text = article.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        # Intentar <main>\n",
    "        main = soup.find(\"main\")\n",
    "        if main:\n",
    "            text = main.get_text(separator=\"\\n\")\n",
    "        else:\n",
    "\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            blocks = []\n",
    "            current_block = []\n",
    "            for p in paragraphs:\n",
    "                current_block.append(p.get_text())\n",
    "                # Separar blocs grans\n",
    "                if len(current_block) > 3:  \n",
    "                    blocks.append(\"\\n\".join(current_block))\n",
    "                    current_block = []\n",
    "            if current_block:\n",
    "                blocks.append(\"\\n\".join(current_block))\n",
    "            # Escollir el bloc més llarg (probable article)\n",
    "            text = max(blocks, key=len) if blocks else \"\"\n",
    "    \n",
    "    # Netejar línies buides\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    clean_text = \"\\n\".join(lines)\n",
    "    \n",
    "\n",
    "    print(clean_text[:1000])  # Mostrar només 1000 caràcters per verificar el contingut\n",
    "    if opcio == \"A\":\n",
    "        title_article_preprocessing_a(clean_text)\n",
    "    elif opcio == \"B\":\n",
    "        title_article_preprocessing_b(clean_text)\n",
    "    elif opcio == \"C\":\n",
    "        generate_summary_b(clean_text)\n",
    "    else:\n",
    "        print(\"Opció no vàlida\")\n",
    "        \n",
    "import requests\n",
    "import trafilatura\n",
    "\n",
    "def scrape_text_advanced(url):\n",
    "    try:\n",
    "        # Descarregar la pàgina web amb un User-Agent vàlid\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        \n",
    "        # Forcem que la resposta sigui interpretada com a UTF-8\n",
    "        response.encoding = 'utf-8' \n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # 2. Li passem el contingut ja descarregat a trafilatura per només obtenir el cos del text, l'article.\n",
    "            clean_text = trafilatura.extract(response.text, include_comments=False)\n",
    "            \n",
    "            if clean_text:\n",
    "                print(clean_text[:1000])  # Mostrar només els primers 1000 caràcters\n",
    "                generate_summary_b(clean_text)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la descarga: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# scrape_text(url, opcio)\n",
    "scrape_text_advanced(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
